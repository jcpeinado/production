{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are we doing?\n",
    "\n",
    "## Objectives \n",
    "\n",
    "\n",
    "* Build a data pipeline that downloads price data from the internet, stores it locally, transforms it into return data, and stores the feature set.\n",
    "    - Getting the data.\n",
    "    - Schemas and index in dask.\n",
    "\n",
    "* Explore the parquet format.\n",
    "    - Reading and writing parquet files.\n",
    "    - Read datasets that are stored in distributed files.\n",
    "    - Discuss dask vs pandas as a small example of big vs small data.\n",
    "    \n",
    "* Discuss the use of environment variables for settings.\n",
    "* Discuss how to use Jupyter notebooks and source code concurrently. \n",
    "* Logging and using a standard logger.\n",
    "\n",
    "## About the Data\n",
    "\n",
    "+ We will download the prices for a list of stocks.\n",
    "+ The source is Yahoo Finance and we will use the API provided by the library yfinance.\n",
    "\n",
    "\n",
    "## Medallion Architecture\n",
    "\n",
    "+ The architecture that we are thinking about is called Medallion by [DataBricks](https://www.databricks.com/glossary/medallion-architecture). It is an ELT type of thinking, although our data is well-structured.\n",
    "\n",
    "![Medallion Architecture (DataBicks)](./images/02_medallion_architecture.png)\n",
    "\n",
    "+ In our case, we would like to optimize the number of times that we download data from the internet. \n",
    "+ Ultimately, we will build a pipeline manager class that will help us control the process of obtaining and transforming our data.\n",
    "\n",
    "![](./images/02_target_pipeline_manager.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Data from Yahoo Finance\n",
    "\n",
    "Yahoo Finance provides information about public stocks in different markets. The library yfinance gives us access to a fair bit of the data in Yahoo Finance. \n",
    "\n",
    "These steps are based on the instructions in:\n",
    "\n",
    "+ [yfinance documentation](https://pypi.org/project/yfinance/)\n",
    "+ [Tutorial in geeksforgeeks.org](https://www.geeksforgeeks.org/get-financial-data-from-yahoo-finance-with-python/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ If required, install: `python -m pip install yfinance`.\n",
    "+ To download the price history of a stock, first use the following setup:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.getenv('SRC_DIR'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few things to notice in the code chunk above:\n",
    "\n",
    "+ Libraries are ordered from high-level to low-level libraries from the package manager (pip in this case, but could be conda, poetry, etc.)\n",
    "+ The command `sys.path.append(\"../05_src/)` will add the `../05_src/` directory to the path in the Notebook's kernel. This way, we can use our modules as part of the notebook.\n",
    "+ Local modules are imported at the end. \n",
    "+ The function `get_logger()` is called with `__name__` as recommended by the documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to download the historical price data for a stock, we could use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Price</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ticker</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>AAPL</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-12-02 00:00:00+00:00</th>\n",
       "      <td>17.175095</td>\n",
       "      <td>19.686787</td>\n",
       "      <td>20.154642</td>\n",
       "      <td>19.672144</td>\n",
       "      <td>19.928572</td>\n",
       "      <td>472544800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-12-03 00:00:00+00:00</th>\n",
       "      <td>17.645271</td>\n",
       "      <td>20.225714</td>\n",
       "      <td>20.227858</td>\n",
       "      <td>19.917143</td>\n",
       "      <td>19.939285</td>\n",
       "      <td>450968000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-12-04 00:00:00+00:00</th>\n",
       "      <td>17.604145</td>\n",
       "      <td>20.178572</td>\n",
       "      <td>20.328215</td>\n",
       "      <td>20.029285</td>\n",
       "      <td>20.196428</td>\n",
       "      <td>377809600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-12-05 00:00:00+00:00</th>\n",
       "      <td>17.694494</td>\n",
       "      <td>20.282143</td>\n",
       "      <td>20.540714</td>\n",
       "      <td>20.228930</td>\n",
       "      <td>20.451786</td>\n",
       "      <td>447580000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-12-06 00:00:00+00:00</th>\n",
       "      <td>17.448975</td>\n",
       "      <td>20.000713</td>\n",
       "      <td>20.241072</td>\n",
       "      <td>19.984644</td>\n",
       "      <td>20.206785</td>\n",
       "      <td>344352400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-25 00:00:00+00:00</th>\n",
       "      <td>193.223389</td>\n",
       "      <td>194.169998</td>\n",
       "      <td>196.270004</td>\n",
       "      <td>193.110001</td>\n",
       "      <td>195.220001</td>\n",
       "      <td>54822100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-26 00:00:00+00:00</th>\n",
       "      <td>191.481918</td>\n",
       "      <td>192.419998</td>\n",
       "      <td>194.759995</td>\n",
       "      <td>191.940002</td>\n",
       "      <td>194.270004</td>\n",
       "      <td>44594000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-29 00:00:00+00:00</th>\n",
       "      <td>190.795288</td>\n",
       "      <td>191.729996</td>\n",
       "      <td>192.199997</td>\n",
       "      <td>189.580002</td>\n",
       "      <td>192.009995</td>\n",
       "      <td>47145600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-30 00:00:00+00:00</th>\n",
       "      <td>187.123291</td>\n",
       "      <td>188.039993</td>\n",
       "      <td>191.800003</td>\n",
       "      <td>187.470001</td>\n",
       "      <td>190.940002</td>\n",
       "      <td>55859400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-31 00:00:00+00:00</th>\n",
       "      <td>183.501022</td>\n",
       "      <td>184.399994</td>\n",
       "      <td>187.100006</td>\n",
       "      <td>184.350006</td>\n",
       "      <td>187.039993</td>\n",
       "      <td>55467800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2558 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Price                       Adj Close       Close        High         Low  \\\n",
       "Ticker                           AAPL        AAPL        AAPL        AAPL   \n",
       "Date                                                                        \n",
       "2013-12-02 00:00:00+00:00   17.175095   19.686787   20.154642   19.672144   \n",
       "2013-12-03 00:00:00+00:00   17.645271   20.225714   20.227858   19.917143   \n",
       "2013-12-04 00:00:00+00:00   17.604145   20.178572   20.328215   20.029285   \n",
       "2013-12-05 00:00:00+00:00   17.694494   20.282143   20.540714   20.228930   \n",
       "2013-12-06 00:00:00+00:00   17.448975   20.000713   20.241072   19.984644   \n",
       "...                               ...         ...         ...         ...   \n",
       "2024-01-25 00:00:00+00:00  193.223389  194.169998  196.270004  193.110001   \n",
       "2024-01-26 00:00:00+00:00  191.481918  192.419998  194.759995  191.940002   \n",
       "2024-01-29 00:00:00+00:00  190.795288  191.729996  192.199997  189.580002   \n",
       "2024-01-30 00:00:00+00:00  187.123291  188.039993  191.800003  187.470001   \n",
       "2024-01-31 00:00:00+00:00  183.501022  184.399994  187.100006  184.350006   \n",
       "\n",
       "Price                            Open     Volume  \n",
       "Ticker                           AAPL       AAPL  \n",
       "Date                                              \n",
       "2013-12-02 00:00:00+00:00   19.928572  472544800  \n",
       "2013-12-03 00:00:00+00:00   19.939285  450968000  \n",
       "2013-12-04 00:00:00+00:00   20.196428  377809600  \n",
       "2013-12-05 00:00:00+00:00   20.451786  447580000  \n",
       "2013-12-06 00:00:00+00:00   20.206785  344352400  \n",
       "...                               ...        ...  \n",
       "2024-01-25 00:00:00+00:00  195.220001   54822100  \n",
       "2024-01-26 00:00:00+00:00  194.270004   44594000  \n",
       "2024-01-29 00:00:00+00:00  192.009995   47145600  \n",
       "2024-01-30 00:00:00+00:00  190.940002   55859400  \n",
       "2024-01-31 00:00:00+00:00  187.039993   55467800  \n",
       "\n",
       "[2558 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "px = yf.download('AAPL', start = \"2013-12-01\", end = \"2024-02-01\")\n",
    "px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Price</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ticker</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>AAPL</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025-01-02 00:00:00+00:00</th>\n",
       "      <td>243.850006</td>\n",
       "      <td>243.850006</td>\n",
       "      <td>249.100006</td>\n",
       "      <td>241.820007</td>\n",
       "      <td>248.929993</td>\n",
       "      <td>55740700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-03 00:00:00+00:00</th>\n",
       "      <td>243.360001</td>\n",
       "      <td>243.360001</td>\n",
       "      <td>244.179993</td>\n",
       "      <td>241.889999</td>\n",
       "      <td>243.360001</td>\n",
       "      <td>40244100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-06 00:00:00+00:00</th>\n",
       "      <td>245.000000</td>\n",
       "      <td>245.000000</td>\n",
       "      <td>247.330002</td>\n",
       "      <td>243.199997</td>\n",
       "      <td>244.309998</td>\n",
       "      <td>45045600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-07 00:00:00+00:00</th>\n",
       "      <td>242.210007</td>\n",
       "      <td>242.210007</td>\n",
       "      <td>245.550003</td>\n",
       "      <td>241.350006</td>\n",
       "      <td>242.979996</td>\n",
       "      <td>40856000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-08 00:00:00+00:00</th>\n",
       "      <td>242.699997</td>\n",
       "      <td>242.699997</td>\n",
       "      <td>243.710007</td>\n",
       "      <td>240.050003</td>\n",
       "      <td>241.919998</td>\n",
       "      <td>37628900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-10 00:00:00+00:00</th>\n",
       "      <td>236.850006</td>\n",
       "      <td>236.850006</td>\n",
       "      <td>240.160004</td>\n",
       "      <td>233.000000</td>\n",
       "      <td>240.009995</td>\n",
       "      <td>61710900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-13 00:00:00+00:00</th>\n",
       "      <td>234.399994</td>\n",
       "      <td>234.399994</td>\n",
       "      <td>234.669998</td>\n",
       "      <td>229.720001</td>\n",
       "      <td>233.529999</td>\n",
       "      <td>49630700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-14 00:00:00+00:00</th>\n",
       "      <td>233.279999</td>\n",
       "      <td>233.279999</td>\n",
       "      <td>236.119995</td>\n",
       "      <td>232.470001</td>\n",
       "      <td>234.750000</td>\n",
       "      <td>39435300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-15 00:00:00+00:00</th>\n",
       "      <td>237.869995</td>\n",
       "      <td>237.869995</td>\n",
       "      <td>238.960007</td>\n",
       "      <td>234.429993</td>\n",
       "      <td>234.639999</td>\n",
       "      <td>39832000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-16 00:00:00+00:00</th>\n",
       "      <td>228.259995</td>\n",
       "      <td>228.259995</td>\n",
       "      <td>238.009995</td>\n",
       "      <td>228.029999</td>\n",
       "      <td>237.350006</td>\n",
       "      <td>71759100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-17 00:00:00+00:00</th>\n",
       "      <td>229.979996</td>\n",
       "      <td>229.979996</td>\n",
       "      <td>232.289993</td>\n",
       "      <td>228.479996</td>\n",
       "      <td>232.119995</td>\n",
       "      <td>68488300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-21 00:00:00+00:00</th>\n",
       "      <td>222.639999</td>\n",
       "      <td>222.639999</td>\n",
       "      <td>224.419998</td>\n",
       "      <td>219.380005</td>\n",
       "      <td>224.000000</td>\n",
       "      <td>98070400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-22 00:00:00+00:00</th>\n",
       "      <td>223.830002</td>\n",
       "      <td>223.830002</td>\n",
       "      <td>224.119995</td>\n",
       "      <td>219.789993</td>\n",
       "      <td>219.789993</td>\n",
       "      <td>64126500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-23 00:00:00+00:00</th>\n",
       "      <td>223.660004</td>\n",
       "      <td>223.660004</td>\n",
       "      <td>227.029999</td>\n",
       "      <td>222.300003</td>\n",
       "      <td>224.740005</td>\n",
       "      <td>60234800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-24 00:00:00+00:00</th>\n",
       "      <td>222.779999</td>\n",
       "      <td>222.779999</td>\n",
       "      <td>225.630005</td>\n",
       "      <td>221.410004</td>\n",
       "      <td>224.779999</td>\n",
       "      <td>54697900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-27 00:00:00+00:00</th>\n",
       "      <td>229.860001</td>\n",
       "      <td>229.860001</td>\n",
       "      <td>232.149994</td>\n",
       "      <td>223.979996</td>\n",
       "      <td>224.020004</td>\n",
       "      <td>94863400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-28 00:00:00+00:00</th>\n",
       "      <td>238.259995</td>\n",
       "      <td>238.259995</td>\n",
       "      <td>240.190002</td>\n",
       "      <td>230.809998</td>\n",
       "      <td>230.850006</td>\n",
       "      <td>75707600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Price                       Adj Close       Close        High         Low  \\\n",
       "Ticker                           AAPL        AAPL        AAPL        AAPL   \n",
       "Date                                                                        \n",
       "2025-01-02 00:00:00+00:00  243.850006  243.850006  249.100006  241.820007   \n",
       "2025-01-03 00:00:00+00:00  243.360001  243.360001  244.179993  241.889999   \n",
       "2025-01-06 00:00:00+00:00  245.000000  245.000000  247.330002  243.199997   \n",
       "2025-01-07 00:00:00+00:00  242.210007  242.210007  245.550003  241.350006   \n",
       "2025-01-08 00:00:00+00:00  242.699997  242.699997  243.710007  240.050003   \n",
       "2025-01-10 00:00:00+00:00  236.850006  236.850006  240.160004  233.000000   \n",
       "2025-01-13 00:00:00+00:00  234.399994  234.399994  234.669998  229.720001   \n",
       "2025-01-14 00:00:00+00:00  233.279999  233.279999  236.119995  232.470001   \n",
       "2025-01-15 00:00:00+00:00  237.869995  237.869995  238.960007  234.429993   \n",
       "2025-01-16 00:00:00+00:00  228.259995  228.259995  238.009995  228.029999   \n",
       "2025-01-17 00:00:00+00:00  229.979996  229.979996  232.289993  228.479996   \n",
       "2025-01-21 00:00:00+00:00  222.639999  222.639999  224.419998  219.380005   \n",
       "2025-01-22 00:00:00+00:00  223.830002  223.830002  224.119995  219.789993   \n",
       "2025-01-23 00:00:00+00:00  223.660004  223.660004  227.029999  222.300003   \n",
       "2025-01-24 00:00:00+00:00  222.779999  222.779999  225.630005  221.410004   \n",
       "2025-01-27 00:00:00+00:00  229.860001  229.860001  232.149994  223.979996   \n",
       "2025-01-28 00:00:00+00:00  238.259995  238.259995  240.190002  230.809998   \n",
       "\n",
       "Price                            Open    Volume  \n",
       "Ticker                           AAPL      AAPL  \n",
       "Date                                             \n",
       "2025-01-02 00:00:00+00:00  248.929993  55740700  \n",
       "2025-01-03 00:00:00+00:00  243.360001  40244100  \n",
       "2025-01-06 00:00:00+00:00  244.309998  45045600  \n",
       "2025-01-07 00:00:00+00:00  242.979996  40856000  \n",
       "2025-01-08 00:00:00+00:00  241.919998  37628900  \n",
       "2025-01-10 00:00:00+00:00  240.009995  61710900  \n",
       "2025-01-13 00:00:00+00:00  233.529999  49630700  \n",
       "2025-01-14 00:00:00+00:00  234.750000  39435300  \n",
       "2025-01-15 00:00:00+00:00  234.639999  39832000  \n",
       "2025-01-16 00:00:00+00:00  237.350006  71759100  \n",
       "2025-01-17 00:00:00+00:00  232.119995  68488300  \n",
       "2025-01-21 00:00:00+00:00  224.000000  98070400  \n",
       "2025-01-22 00:00:00+00:00  219.789993  64126500  \n",
       "2025-01-23 00:00:00+00:00  224.740005  60234800  \n",
       "2025-01-24 00:00:00+00:00  224.779999  54697900  \n",
       "2025-01-27 00:00:00+00:00  224.020004  94863400  \n",
       "2025-01-28 00:00:00+00:00  230.850006  75707600  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yf.download('AAPL', start = '2025-01-01', end = '2025-01-29')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametrize the download\n",
    "\n",
    "+ Generally, we will look to separate every parameter and setting from functions.\n",
    "+ If we had a few stocks, we could cycle through them. We need a place to store the list of tickers (a db or file, for example).\n",
    "+ Store a csv file with a few stock tickers. The location of the file is a setting, the contents of this file are parameters.\n",
    "+ Use **environment variables** to pass parameters.\n",
    "\n",
    "Start by getting a sample of Information Technology stock tickers by applying subindexing and converting the \"ticker\" column from a pandas object to a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all tickers\n",
    "ticker_file = os.getenv(\"TICKERS\")\n",
    "tickers = pd.read_csv(ticker_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can subset our ticker data set using standard indexing techniques. A good reference for this type of data manipulation is Panda's [Documentation](https://pandas.pydata.org/docs/user_guide/indexing.html#indexing-and-selecting-data) and [Cookbook](https://pandas.pydata.org/docs/user_guide/cookbook.html#cookbook-selection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_tech = tickers['GICS Sector'] == 'Information Technology'\n",
    "tech_sector = tickers[idx_tech]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the subset data frame, select one column and convert to list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tech_tickers = tech_sector['ticker'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tech_raw_dt = yf.download(tech_tickers, start = \"2000-01-01\", end = \"2025-01-26\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data that we downloaded combines several stocks and prices into a single row. We want to parse this arrangement into a dataframe that contains observations about a single stock on a given day per row. To do this, we can use the function [`stack()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.stack.html) and re-arrange the indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, check what tech_raw_dt.stack() looks like.\n",
    "tech_raw_dt.stack(future_stack=True).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tech_dt = (tech_raw_dt\n",
    "           .stack(future_stack=True)\n",
    "           .reset_index()\n",
    "           .sort_values(['Ticker', 'Date']))\n",
    "tech_dt.columns.name = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing Data in CSV\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ We have some data. How do we store it?\n",
    "+ We can compare two options, CSV and Parqruet, by measuring their performance:\n",
    "\n",
    "    - Time to save.\n",
    "    - Space required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dir_size(path='.'):\n",
    "    '''Returns the total size of files contained in path.'''\n",
    "    total = 0\n",
    "    with os.scandir(path) as it:\n",
    "        for entry in it:\n",
    "            if entry.is_file():\n",
    "                total += entry.stat().st_size\n",
    "            elif entry.is_dir():\n",
    "                total += get_dir_size(entry.path)\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = os.getenv(\"TEMP_DATA\")\n",
    "os.makedirs(temp, exist_ok=True)\n",
    "stock_path = os.path.join(temp, \"stock_px.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "tech_dt.to_csv(stock_path, index = False)\n",
    "end = time.time()\n",
    "\n",
    "print(f'Writing to dt ({tech_dt.shape})csv took {end - start} seconds.')\n",
    "print(f'Csv file size { os.path.getsize(stock_path)*1e-6 } MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Data to Parquet\n",
    "\n",
    "### Dask \n",
    "\n",
    "We can work with with large data sets and parquet files. In fact, recent versions of pandas support pyarrow data types and future versions will require a pyarrow backend. The pyarrow library is an interface between Python and the Appache Arrow project. The [parquet data format](https://parquet.apache.org/) and [Arrow](https://arrow.apache.org/docs/python/parquet.html) are projects of the Apache Foundation.\n",
    "\n",
    "However, Dask is much more than an interface to Arrow: Dask provides parallel and distributed computing on pandas-like dataframes. It is also relatively easy to use, bridging a gap between pandas and Spark. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px_dd = dd.from_pandas(tech_dt, npartitions = len(tech_tickers))\n",
    "parquet_path = os.path.join(temp, \"stock_px.parquet\")\n",
    "\n",
    "start = time.time()\n",
    "px_dd.to_parquet(parquet_path, engine = \"pyarrow\")\n",
    "end = time.time()\n",
    "\n",
    "print(f'Writing dd ({tech_dt.shape}) to parquet took {end - start} seconds.')\n",
    "print(f'Parquet file size { get_dir_size(parquet_path)*1e-6 } MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parquet files and Dask Dataframes\n",
    "\n",
    "+ Parquet files are immutable: once written, they cannot be modified.\n",
    "+ Dask DataFrames are a useful implementation to manipulate data stored in parquets.\n",
    "+ Parquet and Dask are not the same: parquet is a file format that can be accessed by many applications and programming languages (Python, R, PowerBI, etc.), while Dask is a package in Python to work with large datasets using distributed computation.\n",
    "+ **Dask is not for everything** (see [Dask DataFrames Best Practices](https://docs.dask.org/en/stable/dataframe-best-practices.html)). \n",
    "\n",
    "    - Consider cases suchas small to large joins, where the small dataframe fits in memory, but the large one does not. \n",
    "    - If possible, use pandas: reduce, then use pandas.\n",
    "    - Pandas performance tips apply to Dask.\n",
    "    - Use the index: it is beneficial to have a well-defined index in Dask DataFrames, as it may speed up searching (filtering) the data. A one-dimensional index is allowed.\n",
    "    - Avoid (or minimize) full-data shuffling: indexing is an expensive operations. \n",
    "    - Some joins are more expensive than others. \n",
    "\n",
    "        * Not expensive:\n",
    "\n",
    "            - Join a Dask DataFrame with a pandas DataFrame.\n",
    "            - Join a Dask DataFrame with another Dask DataFrame of a single partition.\n",
    "            - Join Dask DataFrames along their indexes.\n",
    "\n",
    "        * Expensive:\n",
    "\n",
    "            - Join Dask DataFrames along columns that are not their index.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do we store prices?\n",
    "\n",
    "+ We can store our data as a single blob. This can be difficult to maintain, especially because parquet files are immutable.\n",
    "+ Strategy: organize data files by ticker and date. Update only latest month.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLean up before start\n",
    "PRICE_DATA = os.getenv(\"PRICE_DATA\")\n",
    "import shutil\n",
    "if os.path.exists(PRICE_DATA):\n",
    "    shutil.rmtree(PRICE_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ticker in tech_dt['Ticker'].unique():\n",
    "    ticker_dt = tech_dt[tech_dt['Ticker'] == ticker]\n",
    "    ticker_dt = ticker_dt.assign(Year = ticker_dt.Date.dt.year)\n",
    "    for yr in ticker_dt['Year'].unique():\n",
    "        yr_dd = dd.from_pandas(ticker_dt[ticker_dt['Year'] == yr],2)\n",
    "        yr_path = os.path.join(PRICE_DATA, ticker, f\"{ticker}_{yr}\")\n",
    "        os.makedirs(os.path.dirname(yr_path), exist_ok=True)\n",
    "        yr_dd.to_parquet(yr_path, engine = \"pyarrow\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why would we want to store data this way?\n",
    "\n",
    "+ Easier to maintain. We do not update old data, only recent data.\n",
    "+ We can also access all files as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load, Transform and Save "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load\n",
    "\n",
    "+ Parquet files can be read individually or as a collection.\n",
    "+ `dd.read_parquet()` can take a list (collection) of files as input.\n",
    "+ Use `glob` to get the collection of files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "parquet_files = glob(os.path.join(PRICE_DATA, \"**/*.parquet\"), recursive = True)\n",
    "dd_px = dd.read_parquet(parquet_files).set_index(\"Ticker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform\n",
    "\n",
    "+ This transformation step will create a *Features* data set. In our case, features will be stock returns (we obtained prices).\n",
    "+ Dask dataframes work like pandas dataframes: in particular, we can perform groupby and apply operations.\n",
    "+ Notice the use of [an anonymous (lambda) function](https://realpython.com/python-lambda/) in the apply statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_shift = dd_px.groupby('Ticker', group_keys=False).apply(\n",
    "    lambda x: x.assign(Close_lag_1 = x['Close'].shift(1))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_rets = dd_shift.assign(\n",
    "    Returns = lambda x: x['Close']/x['Close_lag_1'] - 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lazy Exection\n",
    "\n",
    "What does `dd_rets` contain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_rets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Dask is a lazy execution framework: commands will not execute until they are required. \n",
    "+ To trigger an execution in dask use `.compute()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_rets.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save\n",
    "\n",
    "+ Apply transformations to calculate daily returns\n",
    "+ Store the enriched data, the silver dataset, in a new directory.\n",
    "+ Should we keep the same namespace? All columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLean up before save\n",
    "FEATURES_DATA = os.getenv(\"FEATURES_DATA\")\n",
    "if os.path.exists(FEATURES_DATA):\n",
    "    shutil.rmtree(FEATURES_DATA)\n",
    "dd_rets.to_parquet(FEATURES_DATA, overwrite = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional: from Jupyter to Command Line\n",
    "\n",
    "+ We have drafted our code in a Jupyter Notebook. \n",
    "+ Finalized code should be written in Python modules.\n",
    "\n",
    "## Object Oriented vs Functional Programming\n",
    "\n",
    "+ We can use classes to keep parameters and functions together.\n",
    "+ We *could* use Object Oriented Programming, but parallelization of data manipulation and modelling tasks benefit from *Functional Programming*.\n",
    "+ An Idea: \n",
    "\n",
    "    - [Data Oriented Programming](https://blog.klipse.tech/dop/2022/06/22/principles-of-dop.html).\n",
    "    - Use the class to bundle together parameters and functions.\n",
    "    - Use stateless operations and treat all data objects as immutable (we do not modify them, we overwrite them).\n",
    "    - Take advantage of [`@staticmethod`](https://realpython.com/instance-class-and-static-methods-demystified/).\n",
    "\n",
    "The code is in `./05_src/`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our original design was:\n",
    "\n",
    "![](./images/02_target_pipeline_manager.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `DataManager` class in `./05_src/data_manager.py` is a simple implementation of the ideas and code discussed in this notebook. The lines below will download data for about 500 stocks from the S&P500. Using this data a few features will be created and stored in the features data set.\n",
    "\n",
    "First, instantiate an object of class `DataManager`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_manager import DataManager\n",
    "dm = DataManager()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download all prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.download_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, add features to the data set and save to a *feature store*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.featurize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsi_participant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
